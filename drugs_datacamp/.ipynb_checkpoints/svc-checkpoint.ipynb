{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "y_df = data[['molecule', 'concentration']]\n",
    "X_df = data.drop(['molecule', 'concentration'], axis=1)\n",
    "spectra = X_df['spectra'].values                                        \n",
    "spectra = np.array([np.array(dd[1:-1].split(',')).astype(float) for dd in spectra])    \n",
    "X_df['spectra'] = spectra.tolist()\n",
    "\n",
    "#Â Loading wavenumbers\n",
    "freqs = pd.read_csv('freq.csv')\n",
    "freqs = freqs['freqs'].values\n",
    "\n",
    "# Target for classification\n",
    "molecule = y_df['molecule'].values\n",
    "# Target for regression\n",
    "concentration = y_df['concentration'].values\n",
    "# \"Raw\" features\n",
    "X = spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal as signal\n",
    "\n",
    "class FeatureExtractorClf():\n",
    "    def __init__(self, window_length=25, polyorder=4):\n",
    "        self.window_length = window_length\n",
    "        self.polyorder = polyorder\n",
    "\n",
    "    def fit(self, X_df, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        XX = np.array([np.array(dd) for dd in X_df['spectra']])\n",
    "        XX = signal.savgol_filter(XX, axis=1, window_length=self.window_length, polyorder=self.polyorder, mode='nearest')\n",
    "        XX = signal.detrend(XX, axis=1)\n",
    "        return XX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import tree\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import xgboost\n",
    "\n",
    "\n",
    "    \n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.n_components = 10\n",
    "        self.n_estimators = 300\n",
    "        self.clf =  xgboost.XGBClassifier(max_depth=20, n_estimators=300, learning_rate=0.2)\n",
    "        parameters = { 'pca__n_components': (8,10, 20, 25),\n",
    "                      'clf__n_estimators':(10,30,100,200,300),\n",
    "                      'clf__max_depth':(10, 15, 20, 30)\n",
    "        }\n",
    "        pipeline = Pipeline([\n",
    "            ('pca', PCA()), \n",
    "            ('clf', self.clf)\n",
    "        ])\n",
    "        self.grid = GridSearchCV(pipeline, parameters, n_jobs=-1,verbose=1)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.grid.fit(X, y)\n",
    "        self.clf = self.grid.best_estimator_\n",
    "        self.best_parameters = self.grid.best_estimator_.get_params()\n",
    "        print \"Best params for XGBoost: \"\n",
    "        for param_name in sorted(self.best_parameters.keys()):\n",
    "            print(\"\\t%s: %r\" % (param_name, self.best_parameters[param_name]))\n",
    "        print \"\"\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 20.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for XGBoost: \n",
      "\tclf: XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=300, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1)\n",
      "\tclf__base_score: 0.5\n",
      "\tclf__colsample_bylevel: 1\n",
      "\tclf__colsample_bytree: 1\n",
      "\tclf__gamma: 0\n",
      "\tclf__learning_rate: 0.05\n",
      "\tclf__max_delta_step: 0\n",
      "\tclf__max_depth: 20\n",
      "\tclf__min_child_weight: 1\n",
      "\tclf__missing: None\n",
      "\tclf__n_estimators: 300\n",
      "\tclf__nthread: -1\n",
      "\tclf__objective: 'multi:softprob'\n",
      "\tclf__reg_alpha: 0\n",
      "\tclf__reg_lambda: 1\n",
      "\tclf__scale_pos_weight: 1\n",
      "\tclf__seed: 0\n",
      "\tclf__silent: True\n",
      "\tclf__subsample: 1\n",
      "\tpca: PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)\n",
      "\tpca__copy: True\n",
      "\tpca__iterated_power: 'auto'\n",
      "\tpca__n_components: 10\n",
      "\tpca__random_state: None\n",
      "\tpca__svd_solver: 'auto'\n",
      "\tpca__tol: 0.0\n",
      "\tpca__whiten: False\n",
      "\tsteps: [('pca', PCA(copy=True, iterated_power='auto', n_components=10, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('clf', XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=20,\n",
      "       min_child_weight=1, missing=None, n_estimators=300, nthread=-1,\n",
      "       objective='multi:softprob', reg_alpha=0, reg_lambda=1,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1))]\n",
      "\n",
      "error = 0.06\n",
      "classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          A       0.92      0.97      0.95        63\n",
      "          B       0.88      0.93      0.90        45\n",
      "          Q       1.00      0.97      0.99        40\n",
      "          R       0.98      0.88      0.93        52\n",
      "\n",
      "avg / total       0.94      0.94      0.94       200\n",
      "\n",
      "confusion matrix:\n",
      " [[61  2  0  0]\n",
      " [ 3 42  0  0]\n",
      " [ 0  0 39  1]\n",
      " [ 2  4  0 46]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "labels = np.array(['A', 'B', 'Q', 'R'])\n",
    "\n",
    "def train_test_model_clf(X_df, y_df, skf_is, FeatureExtractor, Classifier):\n",
    "    train_is, test_is = skf_is\n",
    "    X_train_df = X_df.iloc[train_is].copy()                                  \n",
    "    y_train_df = y_df.iloc[train_is].copy()\n",
    "    y_train_clf = y_train_df['molecule'].values\n",
    "    X_test_df = X_df.iloc[test_is].copy()                                    \n",
    "    y_test_df = y_df.iloc[test_is].copy() \n",
    "    y_test_clf = y_test_df['molecule'].values \n",
    "    # Feature extraction\n",
    "    fe_clf = FeatureExtractor()\n",
    "    fe_clf.fit(X_train_df, y_train_df)\n",
    "    X_train_array_clf = fe_clf.transform(X_train_df)\n",
    "    X_test_array_clf = fe_clf.transform(X_test_df)\n",
    "    # Train\n",
    "    \n",
    "    clf = Classifier()#randomForest\n",
    "    clf.fit(X_train_array_clf, y_train_clf)\n",
    "    \n",
    "    # Test \n",
    "    y_proba_clf = clf.predict_proba(X_test_array_clf)                        \n",
    "    y_pred_clf = labels[np.argmax(y_proba_clf, axis=1)]                      \n",
    "    error = 1 - accuracy_score(y_test_clf, y_pred_clf)                       \n",
    "    print('error = %s' % error)                                                                            \n",
    "    print('classification report:\\n %s' % classification_report(y_test_clf, y_pred_clf))\n",
    "    print('confusion matrix:\\n %s' % confusion_matrix(y_test_clf, y_pred_clf))\n",
    "    \n",
    "\n",
    "skf = ShuffleSplit(n_splits=2, test_size=0.2, random_state=57)  \n",
    "skf_is = list(skf.split(X_df))[0]\n",
    "\n",
    "train_test_model_clf(X_df, y_df, skf_is, FeatureExtractorClf, Classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
